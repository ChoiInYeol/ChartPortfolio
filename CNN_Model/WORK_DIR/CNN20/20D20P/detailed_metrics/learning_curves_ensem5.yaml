model_0:
  train:
    accuracy:
    - 0.5111187624333987
    - 0.5234234798213764
    - 0.528897653130782
    - 0.5338988356078628
    - 0.540030327059249
    - 0.5438316430866825
    - 0.5499561787905346
    - 0.5548808480447394
    - 0.5599202871332583
    - 0.5652657791132814
    - 0.5695261744779712
    - 0.5759498073257933
    loss:
    - 0.9787262719100043
    - 0.871690569240966
    - 0.8129532248846848
    - 0.7722435311663952
    - 0.743744442834166
    - 0.7256258044791385
    - 0.7117104414563004
    - 0.7017766958960769
    - 0.693723989120418
    - 0.6882768891763271
    - 0.6841396047588966
    - 0.6790001964256653
    mcc:
    - 0.011197609897467379
    - 0.034840363489192096
    - 0.04457790391282019
    - 0.053525734979544604
    - 0.06440473036232132
    - 0.07095060975404131
    - 0.0820662299483107
    - 0.09110375437446962
    - 0.10063964103258285
    - 0.11116581464187499
    - 0.1194340284101788
    - 0.13262884028268765
  validate:
    accuracy:
    - 0.5438086813980475
    - 0.5532058200585901
    - 0.5569873974470295
    - 0.561905071046588
    - 0.5621647501805581
    - 0.5641529185500166
    - 0.5595436139220475
    - 0.5512501115808779
    - 0.5664088810263818
    - 0.5594624641926819
    - 0.561718426669047
    - 0.5661492018924117
    loss:
    - 0.7148835416209843
    - 0.7133295564777288
    - 0.7018859880146555
    - 0.6863478444753244
    - 0.6875332059198465
    - 0.6810953259344253
    - 0.6817631579088549
    - 0.6847067991672579
    - 0.6795345238373214
    - 0.681243738665976
    - 0.6801136183732324
    - 0.6827740289331279
    mcc:
    - 0.05519340700705995
    - 0.055593604440715885
    - 0.06590170308643249
    - 0.08516176455969328
    - 0.08243121578858409
    - 0.09395961318822171
    - 0.09766408771760494
    - 0.10606492496826467
    - 0.1024684030960512
    - 0.10604876994661157
    - 0.10791687639713811
    - 0.09409898215712645
model_1:
  train:
    accuracy: []
    loss: []
    mcc: []
  validate:
    accuracy:
    - 0.5315875321555803
    - 0.5430702188608201
    - 0.5342735881975834
    - 0.5593163946798237
    - 0.5577907797677495
    - 0.5626029587191327
    - 0.5612883331034091
    - 0.5625623838544499
    - 0.5665306056204302
    - 0.5540010874063735
    - 0.5668389745920197
    - 0.5621322902888118
    - 0.5660518222171729
    - 0.5678533462090903
    loss:
    - 0.7138425883841572
    - 0.6971700867657838
    - 0.696669264519848
    - 0.6866870498154963
    - 0.6838330159596209
    - 0.6824706185248964
    - 0.6815379823583071
    - 0.6826229272470206
    - 0.6804712743186427
    - 0.6837186401516574
    - 0.680335188158379
    - 0.6882354954652283
    - 0.6808959711272159
    - 0.6807090680509382
    mcc:
    - 0.055885237006441935
    - 0.06482065569514327
    - 0.0800697138348477
    - 0.0810257800157661
    - 0.08360137291455139
    - 0.09099514884575724
    - 0.09688364324252294
    - 0.0855858062036444
    - 0.09869916350228618
    - 0.10469411869979753
    - 0.09881044160792757
    - 0.08522870120575736
    - 0.09784745715858383
    - 0.10161815055224542
model_2:
  train:
    accuracy: []
    loss: []
    mcc: []
  validate:
    accuracy:
    - 0.5405302323316752
    - 0.5489454592668933
    - 0.5379983607754668
    - 0.5579368492806076
    - 0.5586185070072791
    - 0.5611666085093606
    - 0.5639500442266026
    - 0.5524835874672358
    - 0.5661005120547923
    - 0.5629762474742147
    - 0.5625786138003229
    - 0.5643963677381136
    - 0.5677153916691687
    - 0.5572633065268727
    - 0.5661410869194752
    - 0.5654756591386768
    - 0.5674151376705159
    - 0.5652484398964529
    loss:
    - 0.7139217121282895
    - 0.6978208533711233
    - 0.6954000600788228
    - 0.6864530554494257
    - 0.6829623184770289
    - 0.6847647509951472
    - 0.6811736916331775
    - 0.6844693824844389
    - 0.6806712068225494
    - 0.6798036725893974
    - 0.6867090172872852
    - 0.6793481234582083
    - 0.6804602826865078
    - 0.6824641261503083
    - 0.6792966563746962
    - 0.6793489290425841
    - 0.6804111993399222
    - 0.6800868496756965
    mcc:
    - 0.054120199640574344
    - 0.069118807221663
    - 0.08035691243728202
    - 0.08261850481024502
    - 0.08848199583236674
    - 0.08477169956105877
    - 0.09561077268097133
    - 0.10401284199835385
    - 0.09902395228371283
    - 0.10478150387833646
    - 0.08967422275790407
    - 0.10499700109873442
    - 0.10472302478973727
    - 0.1092111655869976
    - 0.10390967621881012
    - 0.11195973557138106
    - 0.10264865225945485
    - 0.10810534051624983
model_3:
  train:
    accuracy: []
    loss: []
    mcc: []
  validate:
    accuracy:
    - 0.546356782900129
    - 0.5534249243278774
    - 0.5562570498827386
    - 0.5576609402007644
    - 0.5615155523456329
    - 0.564818346330815
    - 0.5632196966623116
    - 0.5624244293145282
    - 0.5605417555932452
    - 0.5659706724878073
    - 0.566871434483766
    loss:
    - 0.7128038905902999
    - 0.7038161260325262
    - 0.6875334731063767
    - 0.6840537847646375
    - 0.6815594500597401
    - 0.6815541007613785
    - 0.6805309006085491
    - 0.6803224101107722
    - 0.6813691405854381
    - 0.6812216745292085
    - 0.680549679148406
    mcc:
    - 0.054942951621117
    - 0.06105514809216969
    - 0.07669855286795754
    - 0.08062509306730642
    - 0.09218116859492965
    - 0.09499477545410533
    - 0.10027703132206359
    - 0.10220851588047057
    - 0.11091360953922964
    - 0.09361359693612378
    - 0.0961556955228102
model_4:
  train:
    accuracy: []
    loss: []
    mcc: []
  validate:
    accuracy:
    - 0.534168093549408
    - 0.5538550178935153
    - 0.5565978787460744
    - 0.5515503655795307
    - 0.5618239213172224
    - 0.5562570498827386
    - 0.5648102313578784
    - 0.5665792954580496
    - 0.5461620235496515
    - 0.5677559665338516
    - 0.5658327179478856
    - 0.567934495938456
    - 0.5685431189086985
    - 0.5685593488545716
    - 0.562951902555405
    - 0.5705069423593472
    - 0.5470222106809274
    loss:
    - 0.7126293485264611
    - 0.7027906333652683
    - 0.68917226739627
    - 0.68675418727447
    - 0.6826919940649442
    - 0.6823767494998232
    - 0.6833871666544777
    - 0.6812260095318923
    - 0.6856028223804078
    - 0.6792872022936829
    - 0.6788308312134314
    - 0.6791210202401747
    - 0.6791329078003187
    - 0.6783518001173213
    - 0.6802416724466589
    - 0.6788677766534965
    - 0.6867460301508452
    mcc:
    - 0.05271886386220195
    - 0.06510756400520573
    - 0.07738100497978881
    - 0.09307519634541049
    - 0.09022127959739802
    - 0.09607777005714639
    - 0.09095785303065906
    - 0.09572345756105155
    - 0.1037566483765871
    - 0.10069404607361998
    - 0.10594475738733805
    - 0.10220882584739453
    - 0.10359972366649488
    - 0.10732466865443022
    - 0.10982661843059006
    - 0.108653507139385
    - 0.10646123199236084
